import os
import streamlit as st
import tempfile
import json
import uuid
from datetime import datetime
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_anthropic import ChatAnthropic
from supabase import create_client, Client
from typing import List, Dict, Any, Optional
import numpy as np

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ë¡œì»¬ í™˜ê²½ì—ì„œë§Œ)
load_dotenv()

# Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
@st.cache_resource
def init_supabase():
    """Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ë¡œì»¬: .env, Cloud: st.secrets)"""
    # Streamlit Cloudì˜ secretsë¥¼ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©
    try:
        # Streamlit Cloudì˜ secrets ì‚¬ìš©
        supabase_url = st.secrets.get("SUPABASE_URL") or os.getenv("SUPABASE_URL")
        supabase_key = st.secrets.get("SUPABASE_ANON_KEY") or os.getenv("SUPABASE_ANON_KEY")
    except:
        # secretsê°€ ì—†ìœ¼ë©´ í™˜ê²½ ë³€ìˆ˜ë§Œ ì‚¬ìš©
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_ANON_KEY")
    
    if not supabase_url or not supabase_key:
        return None
    
    try:
        return create_client(supabase_url, supabase_key)
    except Exception as e:
        return None

supabase: Client = init_supabase()

# ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•œ ì»¤ìŠ¤í…€ Retriever í´ë˜ìŠ¤
class SupabaseRetriever:
    """Supabaseë¥¼ ì‚¬ìš©í•œ ë²¡í„° ê²€ìƒ‰ Retriever (ì‚¬ìš©ìë³„ ë¶„ë¦¬)"""
    def __init__(self, supabase_client: Client, user_id: str, session_id: str, embeddings: OpenAIEmbeddings, k: int = 10):
        self.supabase = supabase_client
        self.user_id = user_id
        self.session_id = session_id
        self.embeddings = embeddings
        self.k = k
    
    def invoke(self, query: str) -> List[Any]:
        """ì¿¼ë¦¬ì— ëŒ€í•œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (ì‚¬ìš©ìë³„ í•„í„°ë§)"""
        if self.supabase is None:
            return []
        
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.embeddings.embed_query(query)
            
            # Supabaseì—ì„œ ë²¡í„° ê²€ìƒ‰ (pgvector ì‚¬ìš©, ì‚¬ìš©ìë³„ í•„í„°ë§)
            try:
                result = self.supabase.rpc(
                    'match_documents',
                    {
                        'query_embedding': query_embedding,
                        'match_threshold': 0.7,
                        'match_count': self.k,
                        'user_id': self.user_id,
                        'session_id': self.session_id
                    }
                ).execute()
                
                # ê²°ê³¼ë¥¼ Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                documents = []
                if result.data:
                    for item in result.data:
                        from langchain.schema import Document
                        doc = Document(
                            page_content=item.get('chunk_text', ''),
                            metadata={
                                'source': item.get('file_name', ''),
                                'chunk_index': item.get('chunk_index', 0),
                                'session_id': item.get('session_id', ''),
                                'user_id': item.get('user_id', '')
                            }
                        )
                        documents.append(doc)
                
                return documents
            except Exception as rpc_error:
                # RPC í•¨ìˆ˜ê°€ ì—†ìœ¼ë©´ ì§ì ‘ SQL ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
                return self._search_with_direct_query(query_embedding)
        except Exception as e:
            return []
    
    def _search_with_direct_query(self, query_embedding: List[float]) -> List[Any]:
        """ì§ì ‘ SQL ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•œ ë²¡í„° ê²€ìƒ‰ (ì‚¬ìš©ìë³„ í•„í„°ë§)"""
        try:
            # Supabaseì—ì„œ í•´ë‹¹ ì‚¬ìš©ìì™€ ì„¸ì…˜ì˜ ëª¨ë“  ì„ë² ë”© ê°€ì ¸ì˜¤ê¸°
            result = self.supabase.table("embeddings").select("*").eq("user_id", self.user_id).eq("session_id", self.session_id).execute()
            
            if not result.data:
                return []
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            documents = []
            similarities = []
            
            for item in result.data:
                embedding_raw = item.get('embedding', None)
                
                # embeddingì´ ë¬¸ìì—´ë¡œ ì €ì¥ë˜ì–´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ íŒŒì‹±
                if isinstance(embedding_raw, str):
                    try:
                        import ast
                        embedding = ast.literal_eval(embedding_raw)
                    except:
                        continue
                elif isinstance(embedding_raw, list):
                    embedding = embedding_raw
                else:
                    continue
                
                # ì„ë² ë”© ì°¨ì› í™•ì¸
                if embedding and len(embedding) == len(query_embedding):
                    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                    dot_product = sum(a * b for a, b in zip(query_embedding, embedding))
                    magnitude_a = sum(a * a for a in query_embedding) ** 0.5
                    magnitude_b = sum(b * b for b in embedding) ** 0.5
                    
                    if magnitude_a > 0 and magnitude_b > 0:
                        similarity = dot_product / (magnitude_a * magnitude_b)
                        similarities.append((similarity, item))
            
            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ kê°œ ì„ íƒ
            similarities.sort(key=lambda x: x[0], reverse=True)
            top_items = similarities[:self.k]
            
            # Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            for similarity, item in top_items:
                from langchain.schema import Document
                doc = Document(
                    page_content=item.get('chunk_text', ''),
                    metadata={
                        'source': item.get('file_name', ''),
                        'chunk_index': item.get('chunk_index', 0),
                        'session_id': item.get('session_id', ''),
                        'user_id': item.get('user_id', ''),
                        'similarity': similarity
                    }
                )
                documents.append(doc)
            
            return documents
        except Exception as e:
            import traceback
            st.error(f"ë²¡í„° ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            return []

# ê´€ë ¨ ì§ˆë¬¸ ìƒì„± í•¨ìˆ˜
def generate_followup_questions(prompt: str, response: str, context_text: str, llm_model) -> List[str]:
    """ë‹µë³€ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ í–¥í›„ ë” í•„ìš”í•œ ì§ˆë¬¸ 3ê°œ ìƒì„±"""
    try:
        question_prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ê¸°ë°˜ìœ¼ë¡œ, ì‚¬ìš©ìê°€ ë” ê¹Šì´ ìˆê²Œ ì•Œì•„ë³¼ ìˆ˜ ìˆëŠ” ê´€ë ¨ ì§ˆë¬¸ 3ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ì›ë˜ ì§ˆë¬¸: {prompt}

ë‹µë³€ ë‚´ìš©:
{response[:1000]}

ê´€ë ¨ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸:
{context_text[:500]}

ìš”êµ¬ì‚¬í•­:
- ë‹µë³€ ë‚´ìš©ê³¼ ê´€ë ¨ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë” ê¹Šì´ ìˆëŠ” ì§ˆë¬¸ ìƒì„±
- ê° ì§ˆë¬¸ì€ í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±
- ì§ˆë¬¸ì€ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì´ì–´ì•¼ í•¨
- ì§ˆë¬¸ë§Œ ì¶œë ¥ (ë²ˆí˜¸ë‚˜ ì„¤ëª… ì—†ì´)
- ê° ì§ˆë¬¸ì€ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„

ì˜ˆì‹œ í˜•ì‹:
ì§ˆë¬¸ 1
ì§ˆë¬¸ 2
ì§ˆë¬¸ 3

ê´€ë ¨ ì§ˆë¬¸:"""
        
        questions_text = llm_model.invoke(question_prompt).content.strip()
        
        # ì§ˆë¬¸ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„ë¦¬
        questions = []
        for line in questions_text.split('\n'):
            line = line.strip()
            # ë²ˆí˜¸ë‚˜ ë¶ˆí•„ìš”í•œ ì ‘ë‘ì‚¬ ì œê±°
            if line:
                # "1. ", "ì§ˆë¬¸ 1: ", "- " ë“±ì˜ ì ‘ë‘ì‚¬ ì œê±°
                for prefix in ['1.', '2.', '3.', 'ì§ˆë¬¸ 1:', 'ì§ˆë¬¸ 2:', 'ì§ˆë¬¸ 3:', '-', 'â€¢']:
                    if line.startswith(prefix):
                        line = line[len(prefix):].strip()
                if line and len(line) > 5:  # ë„ˆë¬´ ì§§ì€ ì§ˆë¬¸ ì œì™¸
                    questions.append(line)
        
        # ìµœëŒ€ 3ê°œë§Œ ë°˜í™˜
        return questions[:3]
    except Exception as e:
        return []

# ì„¸ì…˜ ì œëª© ìƒì„± í•¨ìˆ˜ (í‚¤ì›Œë“œ ê¸°ë°˜)
def generate_session_title(chat_history: list, llm_model) -> str:
    """ëŒ€í™” ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì—¬ ì„¸ì…˜ ì œëª© ìƒì„±"""
    if not chat_history or len(chat_history) == 0:
        return "ìƒˆ ì„¸ì…˜"
    
    # ìµœê·¼ ëŒ€í™”ë§Œ ì‚¬ìš© (ì²˜ìŒ 3ê°œ ëŒ€í™”)
    recent_chats = chat_history[:6] if len(chat_history) > 6 else chat_history
    
    # ëŒ€í™” ë‚´ìš© ìš”ì•½
    conversation_text = ""
    for msg in recent_chats:
        role = msg.get("role", "")
        content = msg.get("content", "")
        if role == "user":
            conversation_text += f"ì‚¬ìš©ì: {content[:100]}\n"
        elif role == "assistant":
            conversation_text += f"AI: {content[:200]}\n"
    
    if not conversation_text.strip():
        return "ìƒˆ ì„¸ì…˜"
    
    try:
        prompt = f"""ë‹¤ìŒ ëŒ€í™” ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì£¼ìš” í‚¤ì›Œë“œ 3-5ê°œë¥¼ ì¶”ì¶œí•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ê²°í•œ ì„¸ì…˜ ì œëª©ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ëŒ€í™” ë‚´ìš©:
{conversation_text}

ìš”êµ¬ì‚¬í•­:
- ì£¼ìš” í‚¤ì›Œë“œ 3-5ê°œë¥¼ ë¨¼ì € ì¶”ì¶œí•˜ì„¸ìš”
- í‚¤ì›Œë“œë¥¼ í™œìš©í•˜ì—¬ 20ì ì´ë‚´ì˜ ì œëª©ì„ ìƒì„±í•˜ì„¸ìš”
- ì œëª©ì€ í•œê¸€ë¡œ ì‘ì„±í•˜ì„¸ìš”
- ë”°ì˜´í‘œë‚˜ íŠ¹ìˆ˜ë¬¸ì ì—†ì´ ì‘ì„±í•˜ì„¸ìš”
- ì œëª©ë§Œ ì¶œë ¥í•˜ì„¸ìš” (ì„¤ëª… ì—†ì´)

ì œëª©:"""
        
        title = llm_model.invoke(prompt).content.strip()
        # ë”°ì˜´í‘œ ì œê±°
        title = title.strip('"').strip("'").strip()
        
        # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
        if len(title) > 30:
            title = title[:27] + "..."
        
        return title if title else "ìƒˆ ì„¸ì…˜"
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì²« ë²ˆì§¸ ì‚¬ìš©ì ë©”ì‹œì§€ ì‚¬ìš©
        first_user_msg = next((msg.get("content", "") for msg in chat_history if msg.get("role") == "user"), "")
        if first_user_msg:
            return first_user_msg[:30] + "..." if len(first_user_msg) > 30 else first_user_msg
        return "ìƒˆ ì„¸ì…˜"

# í…Œì´ë¸” ì¡´ì¬ í™•ì¸ ë° ìƒì„± í•¨ìˆ˜
def check_and_create_embeddings_table():
    """embeddings í…Œì´ë¸”ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  user_id ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸"""
    if supabase is None:
        return False
    
    try:
        # í…Œì´ë¸” ì¡´ì¬ í™•ì¸ ì‹œë„
        supabase.table("embeddings").select("id").limit(1).execute()
        
        # user_id ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
        try:
            supabase.table("embeddings").select("user_id").limit(1).execute()
            return True
        except Exception as e:
            error_msg = str(e)
            if "column" in error_msg.lower() and "user_id" in error_msg.lower():
                st.error("""
                âš ï¸ **embeddings í…Œì´ë¸”ì— user_id ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!**
                
                **í•´ê²° ë°©ë²•:**
                1. Supabase ëŒ€ì‹œë³´ë“œì—ì„œ SQL Editorë¥¼ ì—½ë‹ˆë‹¤
                2. `supabase_multi_user_migration.sql` íŒŒì¼ì˜ ë‚´ìš©ì„ ë³µì‚¬í•˜ì—¬ ì‹¤í–‰í•©ë‹ˆë‹¤
                3. ë˜ëŠ” `supabase_multi_user_setup.sql` íŒŒì¼ì„ ì‹¤í–‰í•˜ì—¬ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤
                
                **ì¤‘ìš”:** ê¸°ì¡´ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ë§ˆì´ê·¸ë ˆì´ì…˜ SQLì„ ì‚¬ìš©í•˜ì„¸ìš”.
                """)
                return False
            return False
    except Exception as e:
        error_msg = str(e)
        if "Could not find the table" in error_msg or "PGRST205" in error_msg:
            st.error("""
            âš ï¸ **embeddings í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤!**
            
            Supabaseì— `embeddings` í…Œì´ë¸”ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
            
            **í•´ê²° ë°©ë²•:**
            1. Supabase ëŒ€ì‹œë³´ë“œì—ì„œ SQL Editorë¥¼ ì—½ë‹ˆë‹¤
            2. `supabase_multi_user_setup.sql` íŒŒì¼ì˜ ë‚´ìš©ì„ ë³µì‚¬í•˜ì—¬ ì‹¤í–‰í•©ë‹ˆë‹¤
            """)
            return False
        return False

# ì‚¬ìš©ì ì„¤ì • ê´€ë¦¬ í•¨ìˆ˜
def save_user_api_keys(user_id: str, api_keys: dict):
    """ì‚¬ìš©ì API í‚¤ë¥¼ Supabaseì— ì €ì¥"""
    if supabase is None:
        return False
    
    try:
        # ì•”í˜¸í™”ëœ í˜•íƒœë¡œ ì €ì¥ (ì‹¤ì œë¡œëŠ” ì•”í˜¸í™” ê¶Œì¥)
        settings_data = {
            "user_id": user_id,
            "openai_api_key": api_keys.get("openai", ""),
            "claude_api_key": api_keys.get("claude", ""),
            "gemini_api_key": api_keys.get("gemini", ""),
            "updated_at": datetime.now().isoformat()
        }
        
        # ê¸°ì¡´ ì„¤ì •ì´ ìˆëŠ”ì§€ í™•ì¸
        existing = supabase.table("user_settings").select("*").eq("user_id", user_id).execute()
        
        if existing.data:
            # ì—…ë°ì´íŠ¸
            supabase.table("user_settings").update(settings_data).eq("user_id", user_id).execute()
        else:
            # ìƒˆë¡œ ìƒì„±
            supabase.table("user_settings").insert(settings_data).execute()
        
        return True
    except Exception as e:
        return False

def load_user_api_keys(user_id: str) -> dict:
    """ì‚¬ìš©ì API í‚¤ë¥¼ Supabaseì—ì„œ ë¡œë“œ"""
    if supabase is None:
        return {"openai": "", "claude": "", "gemini": ""}
    
    try:
        result = supabase.table("user_settings").select("*").eq("user_id", user_id).execute()
        
        if result.data and len(result.data) > 0:
            settings = result.data[0]
            return {
                "openai": settings.get("openai_api_key", ""),
                "claude": settings.get("claude_api_key", ""),
                "gemini": settings.get("gemini_api_key", "")
            }
        return {"openai": "", "claude": "", "gemini": ""}
    except Exception as e:
        return {"openai": "", "claude": "", "gemini": ""}

# ì„¸ì…˜ ê´€ë¦¬ í•¨ìˆ˜
def save_embeddings_to_supabase(user_id: str, session_id: str, file_name: str, chunks: List[Any], embeddings_model: OpenAIEmbeddings):
    """ì„ë² ë”©ì„ Supabaseì— ì €ì¥ (ì´ë¯¸ ì¡´ì¬í•˜ë©´ ì¬ì‚¬ìš©, ì‚¬ìš©ìë³„ ë¶„ë¦¬)"""
    if supabase is None:
        st.error("Supabase ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
    if not check_and_create_embeddings_table():
        return False
    
    try:
        # í•´ë‹¹ ì‚¬ìš©ì, ì„¸ì…˜, íŒŒì¼ì˜ ì„ë² ë”©ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        existing = supabase.table("embeddings").select("id").eq("user_id", user_id).eq("session_id", session_id).eq("file_name", file_name).limit(1).execute()
        
        if existing.data and len(existing.data) > 0:
            # ì´ë¯¸ ì„ë² ë”©ì´ ì¡´ì¬í•˜ë©´ ì¬ì‚¬ìš©
            st.info(f"'{file_name}'ì˜ ì„ë² ë”©ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return True
        
        # ìƒˆ ì„ë² ë”© ìƒì„± ë° ì €ì¥
        batch_size = 30
        total_saved = 0
        total_errors = 0
        
        for i in range(0, len(chunks), batch_size):
            batch_chunks = chunks[i:i + batch_size]
            
            try:
                # ê° ì²­í¬ì˜ ì„ë² ë”© ìƒì„±
                texts = [chunk.page_content for chunk in batch_chunks]
                embeddings_list = embeddings_model.embed_documents(texts)
                
                # Supabaseì— ë°°ì¹˜ë¡œ ì €ì¥
                batch_data = []
                for idx, (chunk, embedding) in enumerate(zip(batch_chunks, embeddings_list)):
                    embedding_str = '[' + ','.join(map(str, embedding)) + ']'
                    
                    # í…ìŠ¤íŠ¸ ì •ì œ: null ë¬¸ì ë° ì œì–´ ë¬¸ì ì œê±°
                    cleaned_text = chunk.page_content[:50000]
                    # null ë¬¸ì(\u0000) ì œê±°
                    cleaned_text = cleaned_text.replace('\x00', '')
                    # ë‹¤ë¥¸ ì œì–´ ë¬¸ìë„ ì œê±° (íƒ­, ì¤„ë°”ê¿ˆ, ìºë¦¬ì§€ ë¦¬í„´ì€ ìœ ì§€)
                    cleaned_text = ''.join(char for char in cleaned_text if ord(char) >= 32 or char in '\n\r\t')
                    
                    embedding_data = {
                        "user_id": user_id,
                        "session_id": session_id,
                        "file_name": file_name,
                        "chunk_index": i + idx,
                        "chunk_text": cleaned_text,
                        "embedding": embedding_str,
                        "metadata": json.dumps(chunk.metadata, ensure_ascii=False) if chunk.metadata else "{}"
                    }
                    batch_data.append(embedding_data)
                
                # ë°°ì¹˜ ì‚½ì… ì‹œë„
                if batch_data:
                    try:
                        result = supabase.table("embeddings").insert(batch_data).execute()
                        total_saved += len(batch_data)
                    except Exception as batch_error:
                        # ë°°ì¹˜ ì‚½ì… ì‹¤íŒ¨ ì‹œ í•˜ë‚˜ì”© ì‚½ì…
                        for data in batch_data:
                            try:
                                if isinstance(data["embedding"], list):
                                    data["embedding"] = '[' + ','.join(map(str, data["embedding"])) + ']'
                                
                                # í…ìŠ¤íŠ¸ ì •ì œ (ê°œë³„ ì‚½ì… ì‹œì—ë„ ì ìš©)
                                if "chunk_text" in data:
                                    cleaned_text = data["chunk_text"]
                                    cleaned_text = cleaned_text.replace('\x00', '')
                                    cleaned_text = ''.join(char for char in cleaned_text if ord(char) >= 32 or char in '\n\r\t')
                                    data["chunk_text"] = cleaned_text
                                
                                supabase.table("embeddings").insert(data).execute()
                                total_saved += 1
                            except Exception as single_error:
                                total_errors += 1
                                st.warning(f"ì²­í¬ ì €ì¥ ì‹¤íŒ¨: {str(single_error)}")
                                continue
                
            except Exception as batch_embed_error:
                st.warning(f"ì²­í¬ {i}~{i+batch_size} ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(batch_embed_error)}")
                total_errors += len(batch_chunks)
                continue
        
        if total_saved > 0:
            st.success(f"'{file_name}': {total_saved}ê°œ ì²­í¬ ì„ë² ë”© ì €ì¥ ì™„ë£Œ")
            if total_errors > 0:
                st.warning(f"'{file_name}': {total_errors}ê°œ ì²­í¬ ì €ì¥ ì‹¤íŒ¨")
            return True
        else:
            st.error(f"'{file_name}': ì„ë² ë”© ì €ì¥ ì‹¤íŒ¨ (ëª¨ë“  ì²­í¬ ì €ì¥ ì‹¤íŒ¨)")
            return False
            
    except Exception as e:
        st.error(f"ì„ë² ë”© ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        st.error(traceback.format_exc())
        return False

def load_embeddings_from_supabase(user_id: str, session_id: str) -> bool:
    """ì„¸ì…˜ì˜ ì„ë² ë”©ì„ ë¡œë“œí•˜ì—¬ retriever ìƒì„±"""
    if supabase is None:
        return False
    
    # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
    if not check_and_create_embeddings_table():
        return False
    
    try:
        # í•´ë‹¹ ì‚¬ìš©ìì™€ ì„¸ì…˜ì˜ ì„ë² ë”©ì´ ìˆëŠ”ì§€ í™•ì¸
        result = supabase.table("embeddings").select("file_name").eq("user_id", user_id).eq("session_id", session_id).limit(1).execute()
        
        if result.data and len(result.data) > 0:
            # ì„ë² ë”©ì´ ì¡´ì¬í•˜ë©´ retriever ìƒì„±
            # API í‚¤ëŠ” session_stateì—ì„œ ê°€ì ¸ì˜¤ê¸°
            api_keys = st.session_state.get("api_keys", {"openai": "", "claude": "", "gemini": ""})
            openai_key = api_keys.get("openai", "")
            if not openai_key:
                # í™˜ê²½ ë³€ìˆ˜ì—ì„œë„ ì‹œë„
                openai_key = os.getenv("OPENAI_API_KEY", "")
            
            if openai_key:
                embeddings = OpenAIEmbeddings(openai_api_key=openai_key)
                st.session_state.retriever = SupabaseRetriever(supabase, user_id, session_id, embeddings, k=10)
                return True
        
        return False
    except Exception as e:
        return False

def save_session_to_supabase(user_id: str, session_id: str, llm_model):
    """í˜„ì¬ ì„¸ì…˜ì„ Supabaseì— ì €ì¥ (ê¸°ì¡´ ì„¸ì…˜ ì—…ë°ì´íŠ¸)"""
    if supabase is None:
        return False
    
    try:
        # ì„¸ì…˜ ì œëª© ìƒì„± (ëŒ€í™” ë‚´ìš©ì´ ìˆì„ ë•Œë§Œ)
        title = None
        if st.session_state.chat_history and len(st.session_state.chat_history) > 0:
            title = generate_session_title(st.session_state.chat_history, llm_model)
        
        session_data = {
            "user_id": user_id,
            "session_id": session_id,
            "chat_history": json.dumps(st.session_state.chat_history, ensure_ascii=False),
            "conversation_memory": json.dumps(st.session_state.conversation_memory, ensure_ascii=False),
            "processed_files": json.dumps(st.session_state.processed_files, ensure_ascii=False),
            "metadata": json.dumps({
                "created_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat()
            }, ensure_ascii=False)
        }
        
        # ì œëª©ì´ ìˆìœ¼ë©´ ì¶”ê°€
        if title:
            session_data["title"] = title
        
        # ê¸°ì¡´ ì„¸ì…˜ì´ ìˆëŠ”ì§€ í™•ì¸
        existing = supabase.table("sessions").select("*").eq("user_id", user_id).eq("session_id", session_id).execute()
        
        if existing.data:
            # ì—…ë°ì´íŠ¸
            supabase.table("sessions").update(session_data).eq("user_id", user_id).eq("session_id", session_id).execute()
        else:
            # ìƒˆë¡œ ìƒì„±
            supabase.table("sessions").insert(session_data).execute()
        
        return True
    except Exception as e:
        return False

def save_new_session_to_supabase(user_id: str, llm_model):
    """ìƒˆë¡œìš´ ì„¸ì…˜ìœ¼ë¡œ Supabaseì— ì €ì¥ (í•­ìƒ INSERT, ì²« ì§ˆë¬¸ê³¼ ë‹µë³€ìœ¼ë¡œ ì œëª© ìƒì„±)"""
    if supabase is None:
        return False, None
    
    try:
        # ì²« ì§ˆë¬¸ê³¼ ë‹µë³€ìœ¼ë¡œ ì„¸ì…˜ ì œëª© ìƒì„±
        title = "ìƒˆ ì„¸ì…˜"
        if st.session_state.chat_history and len(st.session_state.chat_history) >= 2:
            # ì²« ë²ˆì§¸ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì²« ë²ˆì§¸ AI ë‹µë³€ ì‚¬ìš©
            first_question = ""
            first_answer = ""
            for msg in st.session_state.chat_history[:2]:
                if msg.get("role") == "user" and not first_question:
                    first_question = msg.get("content", "")[:100]
                elif msg.get("role") == "assistant" and not first_answer:
                    first_answer = msg.get("content", "")[:200]
            
            if first_question and first_answer:
                title_prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ê¸°ë°˜ìœ¼ë¡œ ê°„ê²°í•œ ì„¸ì…˜ ì œëª©ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {first_question}

ë‹µë³€: {first_answer[:500]}

ìš”êµ¬ì‚¬í•­:
- 20ì ì´ë‚´ì˜ ì œëª©ì„ ìƒì„±í•˜ì„¸ìš”
- ì œëª©ì€ í•œê¸€ë¡œ ì‘ì„±í•˜ì„¸ìš”
- ë”°ì˜´í‘œë‚˜ íŠ¹ìˆ˜ë¬¸ì ì—†ì´ ì‘ì„±í•˜ì„¸ìš”
- ì œëª©ë§Œ ì¶œë ¥í•˜ì„¸ìš” (ì„¤ëª… ì—†ì´)

ì œëª©:"""
                try:
                    title = llm_model.invoke(title_prompt).content.strip()
                    title = title.strip('"').strip("'").strip()
                    if len(title) > 30:
                        title = title[:27] + "..."
                except:
                    title = first_question[:30] if first_question else "ìƒˆ ì„¸ì…˜"
        
        # ìƒˆë¡œìš´ session_id ìƒì„±
        new_session_id = str(uuid.uuid4())
        
        session_data = {
            "user_id": user_id,
            "session_id": new_session_id,
            "title": title,
            "chat_history": json.dumps(st.session_state.chat_history, ensure_ascii=False),
            "conversation_memory": json.dumps(st.session_state.conversation_memory, ensure_ascii=False),
            "processed_files": json.dumps(st.session_state.processed_files, ensure_ascii=False),
            "metadata": json.dumps({
                "created_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat()
            }, ensure_ascii=False)
        }
        
        # í•­ìƒ INSERTë§Œ ìˆ˜í–‰
        supabase.table("sessions").insert(session_data).execute()
        
        return True, new_session_id
    except Exception as e:
        return False, None

def load_session_from_supabase(user_id: str, session_id: str):
    """Supabaseì—ì„œ ì„¸ì…˜ ë¡œë“œ"""
    if supabase is None:
        return False
    
    try:
        result = supabase.table("sessions").select("*").eq("user_id", user_id).eq("session_id", session_id).execute()
        
        if result.data:
            session_data = result.data[0]
            
            # í˜„ì¬ ì„¸ì…˜ì˜ ìƒíƒœë¥¼ ì™„ì „íˆ ì´ˆê¸°í™”
            st.session_state.chat_history = []
            st.session_state.conversation_memory = []
            st.session_state.processed_files = []
            st.session_state.retriever = None
            st.session_state.vectorstore = None
            
            # ë¡œë“œí•  ì„¸ì…˜ì˜ ë°ì´í„°ë§Œ ë³µì›
            if session_data.get("chat_history"):
                loaded_history = json.loads(session_data["chat_history"])
                st.session_state.chat_history = loaded_history.copy() if isinstance(loaded_history, list) else []
            else:
                st.session_state.chat_history = []
            
            if session_data.get("conversation_memory"):
                loaded_memory = json.loads(session_data["conversation_memory"])
                st.session_state.conversation_memory = loaded_memory.copy() if isinstance(loaded_memory, list) else []
            else:
                st.session_state.conversation_memory = []
            
            if session_data.get("processed_files"):
                loaded_files = json.loads(session_data["processed_files"])
                st.session_state.processed_files = loaded_files.copy() if isinstance(loaded_files, list) else []
            else:
                st.session_state.processed_files = []
            
            # ì„ë² ë”© ë¡œë“œ
            load_embeddings_from_supabase(user_id, session_id)
            
            return True
        return False
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ìƒíƒœë¥¼ ì´ˆê¸°í™”
        st.session_state.chat_history = []
        st.session_state.conversation_memory = []
        st.session_state.processed_files = []
        st.session_state.retriever = None
        st.session_state.vectorstore = None
        return False

def list_sessions_from_supabase(user_id: str):
    """Supabaseì—ì„œ ì‚¬ìš©ìì˜ ëª¨ë“  ì„¸ì…˜ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    if supabase is None:
        return []
    
    try:
        result = supabase.table("sessions").select("session_id, title, created_at, updated_at").eq("user_id", user_id).order("updated_at", desc=True).limit(50).execute()
        sessions = result.data if result.data else []
        return sessions
    except Exception as e:
        return []

def delete_session_from_supabase(user_id: str, session_id: str):
    """Supabaseì—ì„œ ì„¸ì…˜ ì‚­ì œ"""
    if supabase is None:
        return False
    
    try:
        # ì„¸ì…˜ ì‚­ì œ
        supabase.table("sessions").delete().eq("user_id", user_id).eq("session_id", session_id).execute()
        # ê´€ë ¨ ì„ë² ë”©ë„ ì‚­ì œ
        supabase.table("embeddings").delete().eq("user_id", user_id).eq("session_id", session_id).execute()
        return True
    except Exception as e:
        return False

# ì¸ì¦ í•¨ìˆ˜
def sign_up(email: str, password: str):
    """íšŒì›ê°€ì…"""
    if supabase is None:
        return None, "Supabase ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤."
    
    try:
        response = supabase.auth.sign_up({
            "email": email,
            "password": password
        })
        return response, None
    except Exception as e:
        return None, str(e)

def sign_in(email: str, password: str):
    """ë¡œê·¸ì¸"""
    if supabase is None:
        return None, "Supabase ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤."
    
    try:
        response = supabase.auth.sign_in_with_password({
            "email": email,
            "password": password
        })
        return response, None
    except Exception as e:
        return None, str(e)

def sign_out():
    """ë¡œê·¸ì•„ì›ƒ"""
    if supabase is None:
        return False
    
    try:
        supabase.auth.sign_out()
        return True
    except Exception as e:
        return False

def get_current_user():
    """í˜„ì¬ ë¡œê·¸ì¸í•œ ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
    if supabase is None:
        return None
    
    try:
        user = supabase.auth.get_user()
        return user.user if user else None
    except Exception as e:
        return None

# LLM ëª¨ë¸ ì„ íƒ í•¨ìˆ˜
def get_llm_model(model_name: str, api_keys: dict):
    """ì„ íƒëœ ëª¨ë¸ëª…ì— ë”°ë¼ LLM ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì§€ì •í•œ ëª¨ë¸ëª…ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©)"""
    # OpenAI ëª¨ë¸ (gptë¡œ ì‹œì‘)
    if model_name.startswith("gpt") or "openai" in model_name.lower():
        api_key = api_keys.get("openai", os.getenv("OPENAI_API_KEY"))
        if not api_key:
            return None
        return ChatOpenAI(model=model_name, temperature=1, openai_api_key=api_key)
    # Gemini ëª¨ë¸ (geminië¡œ ì‹œì‘)
    elif model_name.startswith("gemini"):
        api_key = api_keys.get("gemini", os.getenv("GOOGLE_API_KEY"))
        if not api_key:
            return None
        return ChatGoogleGenerativeAI(model=model_name, temperature=1, google_api_key=api_key)
    # Claude ëª¨ë¸ (claudeë¡œ ì‹œì‘)
    elif model_name.startswith("claude"):
        api_key = api_keys.get("claude", os.getenv("ANTHROPIC_API_KEY"))
        if not api_key:
            return None
        return ChatAnthropic(model=model_name, temperature=1, anthropic_api_key=api_key)
    else:
        # ê¸°ë³¸ê°’ìœ¼ë¡œ OpenAI ì‚¬ìš© (ëª¨ë¸ëª… ê·¸ëŒ€ë¡œ)
        api_key = api_keys.get("openai", os.getenv("OPENAI_API_KEY"))
        if not api_key:
            return None
        return ChatOpenAI(model=model_name, temperature=1, openai_api_key=api_key)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="PDF ê¸°ë°˜ ë©€í‹°ìœ ì € ë©€í‹°ì„¸ì…˜ RAG ì±—ë´‡",
    page_icon="ğŸ“š",
    layout="wide"
)

# ì´ˆê¸° ìƒíƒœ ì„¤ì •
if "conversation_memory" not in st.session_state:
    st.session_state.conversation_memory = []

if "retriever" not in st.session_state:
    st.session_state.retriever = None

if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None

if "processed_files" not in st.session_state:
    st.session_state.processed_files = []

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "current_session_id" not in st.session_state:
    st.session_state.current_session_id = str(uuid.uuid4())

if "session_loaded" not in st.session_state:
    st.session_state.session_loaded = False

if "selected_session_index" not in st.session_state:
    st.session_state.selected_session_index = 0

if "selected_llm_model" not in st.session_state:
    st.session_state.selected_llm_model = "gpt-5.1"

if "api_keys" not in st.session_state:
    st.session_state.api_keys = {
        "openai": "",
        "claude": "",
        "gemini": ""
    }

if "user" not in st.session_state:
    st.session_state.user = None

if "api_keys_loaded" not in st.session_state:
    st.session_state.api_keys_loaded = False

# CSS ìŠ¤íƒ€ì¼ (ref.py ì°¸ê³ )
st.markdown("""
<style>
/* í—¤ë”© ìŠ¤íƒ€ì¼ */
h1 {
    font-size: 1.4rem !important;
    font-weight: 600 !important;
    color: #ff69b4 !important; /* ë¶„í™ìƒ‰ */
}
h2 {
    font-size: 1.2rem !important;
    font-weight: 600 !important;
    color: #ffd700 !important; /* ë…¸ë‘ìƒ‰ */
}
h3 {
    font-size: 1.1rem !important;
    font-weight: 600 !important;
    color: #1f77b4 !important; /* ì²­ìƒ‰ */
}
h4 {
    font-size: 1.1rem !important;
    font-weight: 600 !important;
}
h5 {
    font-size: 1rem !important;
    font-weight: 600 !important;
}
h6 {
    font-size: 0.95rem !important;
    font-weight: 600 !important;
}

/* ì±„íŒ… ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ */
.stChatMessage {
    font-size: 0.95rem !important;
    line-height: 1.5 !important;
}

/* ë‹µë³€ ë‚´ìš© ìŠ¤íƒ€ì¼ */
.stChatMessage p {
    font-size: 0.95rem !important;
    line-height: 1.5 !important;
    margin: 0.5rem 0 !important;
}

/* ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼ */
.stChatMessage ul, .stChatMessage ol {
    font-size: 0.95rem !important;
    line-height: 1.5 !important;
    margin: 0.5rem 0 !important;
}

.stChatMessage li {
    font-size: 0.95rem !important;
    line-height: 1.5 !important;
    margin: 0.3rem 0 !important;
}

/* ê°•ì¡° í…ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼ */
.stChatMessage strong, .stChatMessage b {
    font-size: 0.95rem !important;
    font-weight: 600 !important;
}

/* ì¸ìš©ë¬¸ ìŠ¤íƒ€ì¼ */
.stChatMessage blockquote {
    font-size: 0.95rem !important;
    line-height: 1.5 !important;
    margin: 0.5rem 0 !important;
    padding-left: 1rem !important;
    border-left: 3px solid #e0e0e0 !important;
}

/* ì½”ë“œ ìŠ¤íƒ€ì¼ */
.stChatMessage code {
    font-size: 0.9rem !important;
    background-color: #f5f5f5 !important;
    padding: 0.2rem 0.4rem !important;
    border-radius: 3px !important;
}

/* ì „ì²´ í…ìŠ¤íŠ¸ ì¼ê´€ì„± */
.stChatMessage * {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif !important;
}

/* ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
.stButton > button {
    background-color: #ff69b4 !important;
    color: white !important;
    border: none !important;
    border-radius: 5px !important;
    padding: 0.5rem 1rem !important;
    font-weight: bold !important;
}

.stButton > button:hover {
    background-color: #ff1493 !important;
}
</style>
""", unsafe_allow_html=True)

# ì œëª©
st.markdown("""
<div style="text-align: center; margin-top: -4rem; margin-bottom: 0.5rem;">
    <h1 style="font-size: 2.5rem; font-weight: bold; margin: 0;">
        <span style="color: #1f77b4;">PDF</span> 
        <span style="color: #ffffff; font-size: 0.7em;">ê¸°ë°˜</span> 
        <span style="color: #9b59b6;">ë©€í‹°ìœ ì €</span> 
        <span style="color: #9b59b6;">ë©€í‹°ì„¸ì…˜</span> 
        <span style="color: #ffd700;">RAG</span> 
        <span style="color: #d62728; font-size: 0.7em;">ì±—ë´‡</span>
    </h1>
</div>
""", unsafe_allow_html=True)

# ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸
current_user = get_current_user()

# ë¡œê·¸ì¸ë˜ì§€ ì•Šì€ ê²½ìš° ë¡œê·¸ì¸ í™”ë©´ í‘œì‹œ
if not current_user:
    st.markdown("### ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤")
    
    tab1, tab2 = st.tabs(["ë¡œê·¸ì¸", "íšŒì›ê°€ì…"])
    
    with tab1:
        email = st.text_input("ì´ë©”ì¼")
        password = st.text_input("ë¹„ë°€ë²ˆí˜¸", type="password")
        
        if st.button("ë¡œê·¸ì¸"):
            response, error = sign_in(email, password)
            if error:
                st.error(f"ë¡œê·¸ì¸ ì‹¤íŒ¨: {error}")
            else:
                st.success("ë¡œê·¸ì¸ ì„±ê³µ!")
                st.session_state.user = response.user
                st.rerun()
    
    with tab2:
        new_email = st.text_input("ì´ë©”ì¼ (íšŒì›ê°€ì…)")
        new_password = st.text_input("ë¹„ë°€ë²ˆí˜¸ (íšŒì›ê°€ì…)", type="password")
        confirm_password = st.text_input("ë¹„ë°€ë²ˆí˜¸ í™•ì¸", type="password")
        
        if st.button("íšŒì›ê°€ì…"):
            if new_password != confirm_password:
                st.error("ë¹„ë°€ë²ˆí˜¸ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            elif len(new_password) < 6:
                st.error("ë¹„ë°€ë²ˆí˜¸ëŠ” ìµœì†Œ 6ì ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
            else:
                response, error = sign_up(new_email, new_password)
                if error:
                    st.error(f"íšŒì›ê°€ì… ì‹¤íŒ¨: {error}")
                else:
                    st.success("íšŒì›ê°€ì… ì„±ê³µ! ë¡œê·¸ì¸í•´ì£¼ì„¸ìš”.")
    
    st.stop()

# ë¡œê·¸ì¸ëœ ê²½ìš° ë©”ì¸ í™”ë©´ í‘œì‹œ
st.session_state.user = current_user
user_id = current_user.id

# ë¡œê·¸ì¸ í›„ API í‚¤ ìë™ ë¡œë“œ (í•œ ë²ˆë§Œ ì‹¤í–‰)
if not st.session_state.api_keys_loaded:
    loaded_keys = load_user_api_keys(user_id)
    if loaded_keys:
        st.session_state.api_keys = loaded_keys
        st.session_state.api_keys_loaded = True

# ìë™ ë¡œë“œ ì œê±° - ìˆ˜ë™ ë¡œë“œë§Œ ì‚¬ìš©

st.markdown(f"**ì•ˆë…•í•˜ì„¸ìš”, {current_user.email}ë‹˜!**")
st.markdown("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ë‚´ìš©ì— ê´€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”!")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    # ë¡œê·¸ì•„ì›ƒ ë²„íŠ¼
    if st.button("ë¡œê·¸ì•„ì›ƒ"):
        sign_out()
        st.session_state.user = None
        st.session_state.chat_history = []
        st.session_state.conversation_memory = []
        st.session_state.processed_files = []
        st.session_state.retriever = None
        st.session_state.api_keys_loaded = False
        st.rerun()
    
    st.markdown("---")
    
    # API í‚¤ ì…ë ¥ ì„¹ì…˜
    st.markdown('<h2 style="color: #1f77b4;">API í‚¤ ì„¤ì •</h2>', unsafe_allow_html=True)
    
    openai_key = st.text_input("OpenAI API Key", type="password", value=st.session_state.api_keys.get("openai", ""), key="openai_key_input")
    claude_key = st.text_input("Claude API Key", type="password", value=st.session_state.api_keys.get("claude", ""), key="claude_key_input")
    gemini_key = st.text_input("Gemini API Key", type="password", value=st.session_state.api_keys.get("gemini", ""), key="gemini_key_input")
    
    # API í‚¤ ë³€ê²½ ê°ì§€ ë° ì €ì¥
    api_keys_changed = False
    if openai_key != st.session_state.api_keys.get("openai", ""):
        st.session_state.api_keys["openai"] = openai_key
        api_keys_changed = True
    
    if claude_key != st.session_state.api_keys.get("claude", ""):
        st.session_state.api_keys["claude"] = claude_key
        api_keys_changed = True
    
    if gemini_key != st.session_state.api_keys.get("gemini", ""):
        st.session_state.api_keys["gemini"] = gemini_key
        api_keys_changed = True
    
    # ë³€ê²½ëœ ê²½ìš°ì—ë§Œ ì €ì¥
    if api_keys_changed and supabase:
        save_user_api_keys(user_id, st.session_state.api_keys)
    
    st.markdown("---")
    
    # LLM ëª¨ë¸ ì„ íƒ
    st.markdown('<h2 style="color: #1f77b4;">LLM ëª¨ë¸ ì„ íƒ</h2>', unsafe_allow_html=True)
    selected_model = st.selectbox(
        "ëª¨ë¸ ì„ íƒ",
        options=["gpt-5.1", "gemini-3-pro-preview", "claude-sonnet-4-5"],
        index=["gpt-5.1", "gemini-3-pro-preview", "claude-sonnet-4-5"].index(st.session_state.selected_llm_model) if st.session_state.selected_llm_model in ["gpt-5.1", "gemini-3-pro-preview", "claude-sonnet-4-5"] else 0,
        key="llm_model_selectbox"
    )
    st.session_state.selected_llm_model = selected_model
    
    model_info = {
        "gpt-5.1": "OpenAI GPT-5.1",
        "gemini-3-pro-preview": "Google Gemini 3 Pro Preview",
        "claude-sonnet-4-5": "Anthropic Claude Sonnet 4.5"
    }
    st.info(f"í˜„ì¬ ëª¨ë¸: **{model_info.get(selected_model, selected_model)}**")
    
    st.markdown("---")
    
    # ì„¸ì…˜ ê´€ë¦¬ ì„¹ì…˜
    st.markdown('<h2 style="color: #1f77b4;">ì„¸ì…˜ ê´€ë¦¬</h2>', unsafe_allow_html=True)
    
    if supabase:
        # ì„¸ì…˜ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        sessions = list_sessions_from_supabase(user_id)
        
        # ì„¸ì…˜ ì˜µì…˜ ìƒì„± (ì œëª© + ë‚ ì§œ)
        session_options = ["ìƒˆ ì„¸ì…˜ ìƒì„±"]
        for s in sessions:
            title = s.get("title", "ì œëª© ì—†ìŒ")
            date = s.get("updated_at", "")[:10] if s.get("updated_at") else ""
            session_options.append(f"{title} ({date})")
        
        # ì„¸ì…˜ ì„ íƒ (ìë™ ë¡œë“œ ì œê±°)
        selected_session = st.selectbox(
            "ì„¸ì…˜ ì„ íƒ",
            options=session_options,
            index=st.session_state.selected_session_index,
            key="session_selectbox"
        )
        
        # ë²„íŠ¼ë“¤ì„ í•œ ì¤„ì— 2ê°œì”© ë°°ì¹˜
        col1, col2 = st.columns(2)
        
        with col1:
            # ì„¸ì…˜ ì €ì¥ ë²„íŠ¼ (ìƒˆ ì„¸ì…˜ìœ¼ë¡œ INSERT)
            if st.button("ì„¸ì…˜ ì €ì¥", use_container_width=True, type="primary"):
                if not st.session_state.chat_history or len(st.session_state.chat_history) == 0:
                    st.warning("ì €ì¥í•  ëŒ€í™” ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    llm_model = get_llm_model(st.session_state.selected_llm_model, st.session_state.api_keys)
                    if llm_model:
                        success, new_session_id = save_new_session_to_supabase(user_id, llm_model)
                        if success:
                            st.success("ì„¸ì…˜ì´ ìƒˆë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                            st.session_state.current_session_id = new_session_id
                            st.rerun()
                        else:
                            st.error("ì„¸ì…˜ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                    else:
                        st.error("LLM ëª¨ë¸ì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        with col2:
            # ì„¸ì…˜ ë¡œë“œ ë²„íŠ¼
            if st.button("ì„¸ì…˜ ë¡œë“œ", use_container_width=True, type="primary"):
                if selected_session != "ìƒˆ ì„¸ì…˜ ìƒì„±":
                    # ì„ íƒëœ ì„¸ì…˜ ì°¾ê¸°
                    selected_index = session_options.index(selected_session) - 1
                    if 0 <= selected_index < len(sessions):
                        selected_session_data = sessions[selected_index]
                        full_session_id = selected_session_data['session_id']
                        
                        # ì„¸ì…˜ ë¡œë“œ
                        if load_session_from_supabase(user_id, full_session_id):
                            st.session_state.current_session_id = full_session_id
                            st.session_state.session_loaded = True
                            st.session_state.selected_session_index = session_options.index(selected_session)
                            st.success(f"ì„¸ì…˜ '{selected_session_data.get('title', 'ì œëª© ì—†ìŒ')}'ì´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
                            st.rerun()
                        else:
                            st.error("ì„¸ì…˜ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                else:
                    st.warning("ë¡œë“œí•  ì„¸ì…˜ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
        
        col3, col4 = st.columns(2)
        
        with col3:
            # ì„¸ì…˜ ì‚­ì œ ë²„íŠ¼
            if st.button("ì„¸ì…˜ ì‚­ì œ", use_container_width=True, type="secondary"):
                if selected_session != "ìƒˆ ì„¸ì…˜ ìƒì„±" and sessions:
                    # ì„ íƒëœ ì„¸ì…˜ ì°¾ê¸°
                    selected_index = session_options.index(selected_session) - 1
                    if 0 <= selected_index < len(sessions):
                        selected_session_data = sessions[selected_index]
                        full_session_id = selected_session_data['session_id']
                        session_title = selected_session_data.get('title', 'ì œëª© ì—†ìŒ')
                        
                        if delete_session_from_supabase(user_id, full_session_id):
                            st.success(f"ì„¸ì…˜ '{session_title}'ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤!")
                            st.session_state.selected_session_index = 0
                            # í˜„ì¬ ì„¸ì…˜ì´ ì‚­ì œëœ ì„¸ì…˜ì´ë©´ ì´ˆê¸°í™”
                            if st.session_state.current_session_id == full_session_id:
                                st.session_state.current_session_id = str(uuid.uuid4())
                                st.session_state.chat_history = []
                                st.session_state.conversation_memory = []
                                st.session_state.processed_files = []
                                st.session_state.retriever = None
                                st.session_state.vectorstore = None
                            st.rerun()
                        else:
                            st.error("ì„¸ì…˜ ì‚­ì œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                else:
                    st.warning("ì‚­ì œí•  ì„¸ì…˜ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
        
        with col4:
            # í™”ë©´ ì´ˆê¸°í™” ë²„íŠ¼
            if st.button("í™”ë©´ ì´ˆê¸°í™”", use_container_width=True, type="secondary"):
                # ëª¨ë“  ìƒíƒœ ì™„ì „íˆ ì´ˆê¸°í™”
                st.session_state.current_session_id = str(uuid.uuid4())
                st.session_state.chat_history = []
                st.session_state.conversation_memory = []
                st.session_state.processed_files = []
                st.session_state.retriever = None
                st.session_state.vectorstore = None
                st.session_state.session_loaded = False
                st.session_state.session_auto_loaded = False
                st.session_state.selected_session_index = 0
                st.success("í™”ë©´ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.rerun()
        
        st.markdown("---")
        # í˜„ì¬ ì„¸ì…˜ ì œëª© í‘œì‹œ
        current_session_title = "ìƒˆ ì„¸ì…˜"
        if st.session_state.chat_history and len(st.session_state.chat_history) > 0:
            try:
                session_data = supabase.table("sessions").select("title").eq("user_id", user_id).eq("session_id", st.session_state.current_session_id).execute()
                if session_data.data and session_data.data[0].get("title"):
                    current_session_title = session_data.data[0]["title"]
                else:
                    # ì œëª©ì´ ì—†ìœ¼ë©´ ìƒì„±
                    llm_model = get_llm_model(st.session_state.selected_llm_model, st.session_state.api_keys)
                    if llm_model:
                        current_session_title = generate_session_title(st.session_state.chat_history, llm_model)
            except:
                llm_model = get_llm_model(st.session_state.selected_llm_model, st.session_state.api_keys)
                if llm_model:
                    current_session_title = generate_session_title(st.session_state.chat_history, llm_model)
        
        st.info(f"ğŸ“Œ í˜„ì¬ ì„¸ì…˜: **{current_session_title}**")
    else:
        st.warning("Supabaseê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¸ì…˜ ì €ì¥ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    st.markdown("---")
    st.markdown('<h2 style="color: #1f77b4;">PDF íŒŒì¼ ì—…ë¡œë“œ</h2>', unsafe_allow_html=True)
    uploaded_files = st.file_uploader("PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”", type="pdf", accept_multiple_files=True)
    
    if uploaded_files:
        process_button = st.button("íŒŒì¼ ì²˜ë¦¬í•˜ê¸°")
        
        if process_button:
            with st.spinner("PDF íŒŒì¼ì„ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤..."):
                try:
                    # API í‚¤ í™•ì¸
                    if not st.session_state.api_keys.get("openai"):
                        st.error("OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                        st.stop()
                    
                    # ì„ì‹œ íŒŒì¼ ìƒì„± ë° ì²˜ë¦¬
                    temp_dir = tempfile.TemporaryDirectory()
                    
                    all_docs = []
                    new_files = []
                    
                    # ê° íŒŒì¼ ì²˜ë¦¬
                    for uploaded_file in uploaded_files:
                        # ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ ìŠ¤í‚µ
                        if uploaded_file.name in st.session_state.processed_files:
                            continue
                            
                        temp_file_path = os.path.join(temp_dir.name, uploaded_file.name)
                        
                        # ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                        with open(temp_file_path, "wb") as f:
                            f.write(uploaded_file.getbuffer())
                        
                        # PDF ë¡œë” ìƒì„± ë° ë¬¸ì„œ ë¡œë“œ
                        loader = PyPDFLoader(temp_file_path)
                        documents = loader.load()
                        
                        # ë©”íƒ€ë°ì´í„°ì— íŒŒì¼ ì´ë¦„ ì¶”ê°€
                        for doc in documents:
                            doc.metadata["source"] = uploaded_file.name
                        
                        all_docs.extend(documents)
                        new_files.append(uploaded_file.name)
                
                    if not all_docs:
                        st.success("ëª¨ë“  íŒŒì¼ì´ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    else:
                        # í…ìŠ¤íŠ¸ ë¶„í• 
                        text_splitter = RecursiveCharacterTextSplitter(
                            chunk_size=500,
                            chunk_overlap=100,
                            length_function=len
                        )
                        chunks = text_splitter.split_documents(all_docs)
                        
                        # ëª¨ë“  ì²­í¬ë¥¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                        total_chunks = len(chunks)
                        st.info(f"ì´ {total_chunks}ê°œì˜ ì²­í¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
                        
                        # ì„ë² ë”© ë° Supabase ë²¡í„° ì €ì¥
                        embeddings = OpenAIEmbeddings(openai_api_key=st.session_state.api_keys.get("openai"))
                        
                        # ê° íŒŒì¼ë³„ë¡œ ì„ë² ë”© ì €ì¥ (ì´ë¯¸ ì¡´ì¬í•˜ë©´ ì¬ì‚¬ìš©)
                        saved_count = 0
                        for file_name in new_files:
                            # í•´ë‹¹ íŒŒì¼ì˜ ì²­í¬ë§Œ í•„í„°ë§
                            file_chunks = [chunk for chunk in chunks if chunk.metadata.get("source") == file_name]
                            
                            if file_chunks:
                                # Supabaseì— ì„ë² ë”© ì €ì¥ (ì´ë¯¸ ì¡´ì¬í•˜ë©´ ì¬ì‚¬ìš©)
                                if save_embeddings_to_supabase(
                                    user_id,
                                    st.session_state.current_session_id,
                                    file_name,
                                    file_chunks,
                                    embeddings
                                ):
                                    saved_count += 1
                        
                        # ì„ë² ë”© ì €ì¥ í™•ì¸
                        if saved_count == 0 and new_files:
                            st.error("ì„ë² ë”© ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. Supabase ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                        
                        # Supabase ê¸°ë°˜ retriever ìƒì„±
                        st.session_state.retriever = SupabaseRetriever(
                            supabase,
                            user_id,
                            st.session_state.current_session_id,
                            embeddings,
                            k=10
                        )
                        
                        # retrieverê°€ ì œëŒ€ë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
                        if st.session_state.retriever and supabase:
                            # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ìˆ˜í–‰
                            test_result = supabase.table("embeddings").select("id").eq("user_id", user_id).eq("session_id", st.session_state.current_session_id).limit(1).execute()
                            if test_result.data:
                                st.success(f"ì„ë² ë”© ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ íŒŒì¼, ì´ {total_chunks}ê°œ ì²­í¬")
                            else:
                                st.warning("ì„ë² ë”©ì´ ì €ì¥ë˜ì—ˆì§€ë§Œ ê²€ìƒ‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Supabase ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                        
                        # ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ ì—…ë°ì´íŠ¸
                        st.session_state.processed_files.extend(new_files)
                        
                        # íŒŒì¼ ì²˜ë¦¬ í›„ ìë™ ì„¸ì…˜ ì €ì¥
                        if supabase:
                            llm_model = get_llm_model(st.session_state.selected_llm_model, st.session_state.api_keys)
                            if llm_model:
                                save_session_to_supabase(user_id, st.session_state.current_session_id, llm_model)
                                st.success("íŒŒì¼ì´ ì²˜ë¦¬ë˜ì—ˆê³  ì„¸ì…˜ì´ ìë™ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    
                except Exception as e:
                    st.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                    st.error("íŒŒì¼ì´ ì†ìƒë˜ì—ˆê±°ë‚˜ ì§€ì›ë˜ì§€ ì•ŠëŠ” í˜•ì‹ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    # ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ í‘œì‹œ
    if st.session_state.processed_files:
        st.markdown('<h3 style="color: #ffd700;">ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡</h3>', unsafe_allow_html=True)
        for file in st.session_state.processed_files:
            st.write(f"- {file}")
    
    # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.chat_history = []
        st.session_state.conversation_memory = []
        # ì´ˆê¸°í™” í›„ ìë™ ì„¸ì…˜ ì €ì¥
        if supabase:
            llm_model = get_llm_model(st.session_state.selected_llm_model, st.session_state.api_keys)
            if llm_model:
                save_session_to_supabase(user_id, st.session_state.current_session_id, llm_model)
        st.rerun()
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í‘œì‹œ
    if st.session_state.processed_files:
        st.subheader("ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ")
        st.info(f"ì²˜ë¦¬ëœ íŒŒì¼ ìˆ˜: {len(st.session_state.processed_files)}")
        st.info(f"ëŒ€í™” ê¸°ë¡ ìˆ˜: {len(st.session_state.chat_history)}")

# ëŒ€í™” ë‚´ìš© í‘œì‹œ (chat_historyê°€ ìˆì„ ë•Œë§Œ)
# chat_historyê°€ ë¹„ì–´ìˆê±°ë‚˜ Noneì´ë©´ ì•„ë¬´ê²ƒë„ í‘œì‹œí•˜ì§€ ì•ŠìŒ
chat_history = st.session_state.get("chat_history", [])
if chat_history and isinstance(chat_history, list) and len(chat_history) > 0:
    for message in chat_history:
        if isinstance(message, dict) and "role" in message and "content" in message:
            with st.chat_message(message["role"]):
                st.write(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì˜ì—­
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.chat_history.append({"role": "user", "content": prompt})
    
    with st.chat_message("user"):
        st.write(prompt)
    
    if st.session_state.retriever is None:
        with st.chat_message("assistant"):
            st.write("ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")
        st.session_state.chat_history.append({"role": "assistant", "content": "ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ì£¼ì„¸ìš”."})
    else:
        try:
            # LLM ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
            llm_model = get_llm_model(st.session_state.selected_llm_model, st.session_state.api_keys)
            if not llm_model:
                st.error("API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                st.stop()
            
            # RAG ê²€ìƒ‰ (ìƒìœ„ 3ê°œ ë¬¸ì„œë§Œ ì‚¬ìš©)
            retrieved_docs = st.session_state.retriever.invoke(prompt)
            
            if not retrieved_docs:
                response = f"ì£„ì†¡í•©ë‹ˆë‹¤. '{prompt}'ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                with st.chat_message("assistant"):
                    st.write(response)
                st.session_state.chat_history.append({"role": "assistant", "content": response})
            else:
                # ìƒìœ„ 3ê°œ ë¬¸ì„œë§Œ ì‚¬ìš©
                top_docs = retrieved_docs[:3]
                
                # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                context_text = ""
                max_context_length = 8000
                current_length = 0
                
                for i, doc in enumerate(top_docs):
                    doc_text = f"[ë¬¸ì„œ {i+1}]\n{doc.page_content}\n\n"
                    if current_length + len(doc_text) > max_context_length:
                        st.warning(f"í† í° ì œí•œìœ¼ë¡œ ì¸í•´ ë¬¸ì„œ {i+1}ê°œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                        break
                    context_text += doc_text
                    current_length += len(doc_text)
                
                # ê³¼ê±° ëŒ€í™” ë§¥ë½ êµ¬ì„±
                conversation_context = ""
                if st.session_state.conversation_memory:
                    conversation_context = "\n\n=== ì´ì „ ëŒ€í™” ë§¥ë½ ===\n"
                    recent_conversations = st.session_state.conversation_memory[-50:]
                    for conv in recent_conversations:
                        conversation_context += f"{conv}\n"
                    conversation_context += "=== ëŒ€í™” ë§¥ë½ ë ===\n"
                
                # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
                system_prompt = f"""
                ì§ˆë¬¸: {prompt}
                
                ê´€ë ¨ ë¬¸ì„œ:
                {context_text}{conversation_context}
                
                ìœ„ ë¬¸ì„œ ë‚´ìš©ê³¼ ì´ì „ ëŒ€í™” ë§¥ë½ì„ ëª¨ë‘ ê³ ë ¤í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
                ì´ì „ ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ ë‚´ìš©ì´ ìˆë‹¤ë©´ ê·¸ê²ƒì„ ì°¸ì¡°í•˜ì—¬ ë” ì •í™•í•˜ê³  ë§¥ë½ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
                
                ë‹µë³€ í˜•ì‹:
                - ë‹µë³€ì€ ë°˜ë“œì‹œ í—¤ë”©(# ## ###)ì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°í™”í•˜ì„¸ìš”
                - ì£¼ìš” ì£¼ì œëŠ” # (H1)ë¡œ, ì„¸ë¶€ ë‚´ìš©ì€ ## (H2)ë¡œ, êµ¬ì²´ì  ì„¤ëª…ì€ ### (H3)ë¡œ êµ¬ë¶„í•˜ì„¸ìš”
                - ë‹µë³€ì´ ê¸¸ê±°ë‚˜ ë³µì¡í•œ ê²½ìš° ì—¬ëŸ¬ í—¤ë”©ì„ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì´ì„¸ìš”
                - ë‹µë³€ì€ ì„œìˆ í˜•ìœ¼ë¡œ ì‘ì„±í•˜ë˜ ì¡´ëŒ€ë§ì„ ì‚¬ìš©í•˜ì„¸ìš”
                - ê°œì¡°ì‹ì´ë‚˜ ë¶ˆì™„ì „í•œ ë¬¸ì¥ì„ ì‚¬ìš©í•˜ì§€ ë§ê³ , ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ì„œìˆ í•˜ì„¸ìš”
                
                ì£¼ì˜ì‚¬í•­:
                - ë‹µë³€ ì¤‘ê°„ì— (ë¬¸ì„œ1), (ë¬¸ì„œ2) ê°™ì€ ì°¸ì¡° í‘œì‹œë¥¼ í•˜ì§€ ë§ˆì„¸ìš”
                - "ì°¸ì¡° ë¬¸ì„œ:", "ì œê³µëœ ë¬¸ì„œ", "ë¬¸ì„œ 1, ë¬¸ì„œ 2" ê°™ì€ ë¬¸êµ¬ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
                - ë‹µë³€ì€ ìˆœìˆ˜í•œ ë‚´ìš©ë§Œ í¬í•¨í•˜ê³ , ì°¸ì¡° ê´€ë ¨ ë¬¸êµ¬ëŠ” ì „í˜€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
                - ë‹µë³€ ëì— ì°¸ì¡° ì •ë³´ë‚˜ ì¶œì²˜ ê´€ë ¨ ë¬¸êµ¬ë¥¼ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”
                """
                
                # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ë‹µë³€ ìƒì„± ë° í‘œì‹œ
                with st.chat_message("assistant"):
                    # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìˆ˜ì§‘ì„ ìœ„í•œ ë³€ìˆ˜
                    full_response = ""
                    
                    # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ í‘œì‹œí•  ë¹ˆ ì»¨í…Œì´ë„ˆ ìƒì„±
                    message_placeholder = st.empty()
                    
                    # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ë° ì‹¤ì‹œê°„ í‘œì‹œ
                    stream = llm_model.stream(system_prompt)
                    for chunk in stream:
                        if hasattr(chunk, 'content'):
                            chunk_text = chunk.content
                        elif hasattr(chunk, 'text'):
                            chunk_text = chunk.text
                        else:
                            chunk_text = str(chunk)
                        
                        if chunk_text:
                            full_response += chunk_text
                            # ì‹¤ì‹œê°„ìœ¼ë¡œ ì‘ë‹µ ì—…ë°ì´íŠ¸
                            message_placeholder.markdown(full_response + "â–Œ")
                    
                    # ìµœì¢… ì‘ë‹µ í‘œì‹œ (ì»¤ì„œ ì œê±°)
                    message_placeholder.markdown(full_response)
                    
                    # ì „ì²´ ì‘ë‹µì„ ë³€ìˆ˜ì— ì €ì¥ (ë‚˜ì¤‘ì— ì‚¬ìš©)
                    response = full_response
                
                # ê´€ë ¨ ì§ˆë¬¸ 3ê°œ ìƒì„± (ë¬¸ì„œë¥¼ ì°¾ì€ ê²½ìš°ì—ë§Œ)
                followup_questions = []
                if retrieved_docs and response:
                    followup_questions = generate_followup_questions(prompt, response, context_text, llm_model)
                
                # ê´€ë ¨ ì§ˆë¬¸ì´ ìˆìœ¼ë©´ ì¶”ê°€ í‘œì‹œ
                if followup_questions:
                    with st.chat_message("assistant"):
                        st.markdown("---")
                        st.markdown("### ğŸ’¡ ë” ì•Œì•„ë³´ê¸°\n")
                        st.markdown("ë‹¤ìŒ ì§ˆë¬¸ë“¤ë„ ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n")
                        for i, question in enumerate(followup_questions, 1):
                            st.markdown(f"{i}. {question}")
                        
                        # ê´€ë ¨ ì§ˆë¬¸ì„ ì „ì²´ ì‘ë‹µì— ì¶”ê°€
                        response += "\n\n---\n\n"
                        response += "### ğŸ’¡ ë” ì•Œì•„ë³´ê¸°\n\n"
                        response += "ë‹¤ìŒ ì§ˆë¬¸ë“¤ë„ ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\n"
                        for i, question in enumerate(followup_questions, 1):
                            response += f"{i}. {question}\n"
                
                # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
                st.session_state.chat_history.append({"role": "assistant", "content": response})
                
                # ëŒ€í™” ë§¥ë½ ë©”ëª¨ë¦¬ì— ì¶”ê°€
                if "conversation_memory" not in st.session_state:
                    st.session_state.conversation_memory = []
                
                st.session_state.conversation_memory.append(f"ì‚¬ìš©ì: {prompt}")
                st.session_state.conversation_memory.append(f"AI: {response}")
                if len(st.session_state.conversation_memory) > 100:
                    st.session_state.conversation_memory = st.session_state.conversation_memory[-100:]
                
                # ëŒ€í™” í›„ ìë™ìœ¼ë¡œ Supabaseì— ì„¸ì…˜ ì €ì¥ (ë°±ê·¸ë¼ìš´ë“œ, rerun ì—†ì´)
                if supabase:
                    try:
                        llm_model = get_llm_model(st.session_state.selected_llm_model, st.session_state.api_keys)
                        if llm_model:
                            save_session_to_supabase(user_id, st.session_state.current_session_id, llm_model)
                    except:
                        pass  # ì €ì¥ ì‹¤íŒ¨í•´ë„ ëŒ€í™”ëŠ” ê³„ì† ì§„í–‰
                
        except Exception as e:
            with st.chat_message("assistant"):
                st.write(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            st.session_state.chat_history.append({"role": "assistant", "content": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"})
